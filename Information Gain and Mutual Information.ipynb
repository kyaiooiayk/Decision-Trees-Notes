{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "**What?** Information Gain and Mutual Information\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some definitions\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- **Information gain** is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. Information gain is calculated by comparing the entropy of the dataset before and after a transformation.\n",
    "\n",
    "- **Mutual information** calculates the statistical dependence between two variables and is the name given to information gain when applied to variable selection.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Information Gain, or IG for short, measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable. \n",
    "\n",
    "- A larger information gain suggests a lower entropy group or groups of samples, and hence less surprise.\n",
    "\n",
    "- You might recall that information quantifies how surprising an event is in bits. Lower probability events have more information, higher probability events have less information. Entropy quantifies how much information there is in a random variable, or more specifically its probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.\n",
    "\n",
    "    - **Low probability** events are more surprising thus have a larger amount of information -> smaller entropy?\n",
    "    - **High probablity** events are less surprising thus have a smaller amouunt of information -> larger entropy?\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Entropy: 0.934 bits\n",
      "Group1 Entropy: 0.544 bits\n",
      "Group2 Entropy: 1.000 bits\n",
      "Information Gain: 0.117 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate the entropy for the split in the dataset\n",
    "def entropy(class0, class1):\n",
    "    return -(class0 * log2(class0) + class1 * log2(class1))\n",
    "\n",
    "\n",
    "# split of the main dataset\n",
    "class0 = 13 / 20\n",
    "class1 = 7 / 20\n",
    "# calculate entropy before the change\n",
    "s_entropy = entropy(class0, class1)\n",
    "print('Dataset Entropy: %.3f bits' % s_entropy)\n",
    "\n",
    "# split 1 (split via value1)\n",
    "s1_class0 = 7 / 8\n",
    "s1_class1 = 1 / 8\n",
    "# calculate the entropy of the first group\n",
    "s1_entropy = entropy(s1_class0, s1_class1)\n",
    "print('Group1 Entropy: %.3f bits' % s1_entropy)\n",
    "\n",
    "# split 2  (split via value2)\n",
    "s2_class0 = 6 / 12\n",
    "s2_class1 = 6 / 12\n",
    "# calculate the entropy of the second group\n",
    "s2_entropy = entropy(s2_class0, s2_class1)\n",
    "print('Group2 Entropy: %.3f bits' % s2_entropy)\n",
    "\n",
    "# calculate the information gain\n",
    "gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)\n",
    "print('Information Gain: %.3f bits' % gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. \n",
    "\n",
    "- The mutual information between two random variables X and Y can be stated formally as follows: `I(X ; Y) = H(X) – H(X | Y)`\n",
    "\n",
    "- It measures the average reduction in uncertainty about x that results from learning the value of y; or **vice versa**, the average amount of information that x conveys about y.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation btw the two\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "-  Notice the similarity in the way that the mutual information is calculated and the way that information gain is calculated; they are equivalent:\n",
    "\n",
    "    - `I(X ; Y) = H(X) – H(X | Y)`\n",
    "    - `IG(S, a) = H(S) – H(S | a)`\n",
    "\n",
    "- As such, mutual information is sometimes used as a synonym for information gain. Technically, they calculate the same quantity if applied to the same data.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "- https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
